{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "QweEMtpYQmP92P9XUkU48",
   "metadata": {},
   "source": [
    "# Récupération des données du S&P500 et sauvegarde dans un CSV\n",
    "\n",
    "Ce notebook récupère la liste des tickers du S&P500 depuis Wikipedia, appelle l'API Yahoo Finance pour récupérer un maximum d'informations sur chaque stock, y compris les données historiques pour une période spécifiée, et sauvegarde le tout dans un fichier CSV.\n",
    "\n",
    "Pour indiquer la période souhaitée, modifiez les variables `start_date` et `end_date` dans la cellule suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "405b70dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import yfinance as yf\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5bf3cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recuperer_tickers_sp500():\n",
    "    import pandas as pd\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    tables = pd.read_html(url, header=0)\n",
    "    df = tables[0]\n",
    "    tickers = df['Symbol'].tolist()\n",
    "    return tickers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R1Gea7zPQNSbcH9nOW4MW",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement de MMM...\n",
      "✅ Données traitées pour MMM\n",
      "Traitement de AOS...\n",
      "✅ Données traitées pour AOS\n",
      "Traitement de ABT...\n",
      "✅ Données traitées pour ABT\n",
      "Traitement de ABBV...\n",
      "✅ Données traitées pour ABBV\n",
      "Traitement de ACN...\n",
      "✅ Données traitées pour ACN\n",
      "Traitement de ADBE...\n",
      "✅ Données traitées pour ADBE\n",
      "Traitement de AMD...\n",
      "✅ Données traitées pour AMD\n",
      "Traitement de AES...\n",
      "✅ Données traitées pour AES\n",
      "Traitement de AFL...\n",
      "✅ Données traitées pour AFL\n",
      "Traitement de A...\n",
      "✅ Données traitées pour A\n",
      "Traitement de APD...\n",
      "✅ Données traitées pour APD\n",
      "Traitement de ABNB...\n",
      "✅ Données traitées pour ABNB\n",
      "Traitement de AKAM...\n",
      "✅ Données traitées pour AKAM\n",
      "Traitement de ALB...\n",
      "✅ Données traitées pour ALB\n",
      "Traitement de ARE...\n",
      "✅ Données traitées pour ARE\n",
      "Traitement de ALGN...\n",
      "✅ Données traitées pour ALGN\n",
      "Traitement de ALLE...\n",
      "✅ Données traitées pour ALLE\n",
      "Traitement de LNT...\n",
      "✅ Données traitées pour LNT\n",
      "Traitement de ALL...\n",
      "✅ Données traitées pour ALL\n",
      "Traitement de GOOGL...\n",
      "✅ Données traitées pour GOOGL\n",
      "Traitement de GOOG...\n",
      "✅ Données traitées pour GOOG\n",
      "Traitement de MO...\n",
      "✅ Données traitées pour MO\n",
      "Traitement de AMZN...\n",
      "✅ Données traitées pour AMZN\n",
      "Traitement de AMCR...\n",
      "✅ Données traitées pour AMCR\n",
      "Traitement de AEE...\n",
      "✅ Données traitées pour AEE\n",
      "Traitement de AEP...\n",
      "✅ Données traitées pour AEP\n",
      "Traitement de AXP...\n",
      "✅ Données traitées pour AXP\n",
      "Traitement de AIG...\n",
      "✅ Données traitées pour AIG\n",
      "Traitement de AMT...\n",
      "✅ Données traitées pour AMT\n",
      "Traitement de AWK...\n",
      "✅ Données traitées pour AWK\n",
      "Traitement de AMP...\n",
      "✅ Données traitées pour AMP\n",
      "Traitement de AME...\n",
      "✅ Données traitées pour AME\n",
      "Traitement de AMGN...\n",
      "✅ Données traitées pour AMGN\n",
      "Traitement de APH...\n",
      "✅ Données traitées pour APH\n",
      "Traitement de ADI...\n",
      "✅ Données traitées pour ADI\n",
      "Traitement de ANSS...\n",
      "✅ Données traitées pour ANSS\n",
      "Traitement de AON...\n",
      "✅ Données traitées pour AON\n",
      "Traitement de APA...\n",
      "✅ Données traitées pour APA\n",
      "Traitement de APO...\n",
      "✅ Données traitées pour APO\n",
      "Traitement de AAPL...\n",
      "✅ Données traitées pour AAPL\n",
      "Traitement de AMAT...\n",
      "✅ Données traitées pour AMAT\n",
      "Traitement de APTV...\n",
      "✅ Données traitées pour APTV\n",
      "Traitement de ACGL...\n",
      "✅ Données traitées pour ACGL\n",
      "Traitement de ADM...\n",
      "✅ Données traitées pour ADM\n",
      "Traitement de ANET...\n",
      "✅ Données traitées pour ANET\n",
      "Traitement de AJG...\n",
      "✅ Données traitées pour AJG\n",
      "Traitement de AIZ...\n",
      "✅ Données traitées pour AIZ\n",
      "Traitement de T...\n",
      "✅ Données traitées pour T\n",
      "Traitement de ATO...\n",
      "✅ Données traitées pour ATO\n",
      "Traitement de ADSK...\n",
      "✅ Données traitées pour ADSK\n",
      "Traitement de ADP...\n",
      "✅ Données traitées pour ADP\n",
      "Traitement de AZO...\n",
      "✅ Données traitées pour AZO\n",
      "Traitement de AVB...\n",
      "✅ Données traitées pour AVB\n",
      "Traitement de AVY...\n",
      "✅ Données traitées pour AVY\n",
      "Traitement de AXON...\n",
      "✅ Données traitées pour AXON\n",
      "Traitement de BKR...\n",
      "✅ Données traitées pour BKR\n",
      "Traitement de BALL...\n",
      "✅ Données traitées pour BALL\n",
      "Traitement de BAC...\n",
      "✅ Données traitées pour BAC\n",
      "Traitement de BAX...\n",
      "✅ Données traitées pour BAX\n",
      "Traitement de BDX...\n",
      "✅ Données traitées pour BDX\n",
      "Traitement de BRK.B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$BRK.B: possibly delisted; no timezone found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Données traitées pour BRK.B\n",
      "Traitement de BBY...\n",
      "✅ Données traitées pour BBY\n",
      "Traitement de TECH...\n",
      "✅ Données traitées pour TECH\n",
      "Traitement de BIIB...\n",
      "✅ Données traitées pour BIIB\n",
      "Traitement de BLK...\n",
      "✅ Données traitées pour BLK\n",
      "Traitement de BX...\n",
      "✅ Données traitées pour BX\n",
      "Traitement de BK...\n",
      "✅ Données traitées pour BK\n",
      "Traitement de BA...\n",
      "✅ Données traitées pour BA\n",
      "Traitement de BKNG...\n",
      "✅ Données traitées pour BKNG\n",
      "Traitement de BSX...\n",
      "✅ Données traitées pour BSX\n",
      "Traitement de BMY...\n",
      "✅ Données traitées pour BMY\n",
      "Traitement de AVGO...\n",
      "✅ Données traitées pour AVGO\n",
      "Traitement de BR...\n",
      "✅ Données traitées pour BR\n",
      "Traitement de BRO...\n",
      "✅ Données traitées pour BRO\n",
      "Traitement de BF.B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$BF.B: possibly delisted; no price data found  (1d 2020-01-01 -> 2025-04-03)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Données traitées pour BF.B\n",
      "Traitement de BLDR...\n",
      "✅ Données traitées pour BLDR\n",
      "Traitement de BG...\n",
      "✅ Données traitées pour BG\n",
      "Traitement de BXP...\n",
      "✅ Données traitées pour BXP\n",
      "Traitement de CHRW...\n",
      "✅ Données traitées pour CHRW\n",
      "Traitement de CDNS...\n",
      "✅ Données traitées pour CDNS\n",
      "Traitement de CZR...\n",
      "✅ Données traitées pour CZR\n",
      "Traitement de CPT...\n",
      "✅ Données traitées pour CPT\n",
      "Traitement de CPB...\n",
      "✅ Données traitées pour CPB\n",
      "Traitement de COF...\n",
      "✅ Données traitées pour COF\n",
      "Traitement de CAH...\n",
      "✅ Données traitées pour CAH\n",
      "Traitement de KMX...\n",
      "✅ Données traitées pour KMX\n"
     ]
    }
   ],
   "source": [
    "def convert_date(value):\n",
    "    \"\"\"Convertit une valeur de date :\n",
    "       - Si c'est un timestamp Unix (en secondes ou millisecondes), retourne 'YYYY-MM-DD'.\n",
    "       - Si la valeur est vide, 'N/A' ou 'null', renvoie None.\n",
    "       - Sinon, retourne la valeur telle quelle.\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    if not isinstance(value, str):\n",
    "        value = str(value)\n",
    "    if not value or value.strip() in [\"\", \"N/A\", \"null\"]:\n",
    "        return None\n",
    "    if value.isdigit():\n",
    "        try:\n",
    "            # Si le timestamp a plus de 10 chiffres, on suppose qu'il est en millisecondes\n",
    "            if len(value) > 10:\n",
    "                ts = int(value) / 1000\n",
    "            else:\n",
    "                ts = int(value)\n",
    "            return datetime.fromtimestamp(ts).strftime(\"%Y-%m-%d\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la conversion du timestamp {value}: {e}\")\n",
    "            return None\n",
    "    return value\n",
    "\n",
    "def merge_historical(existing_list, new_list):\n",
    "    \"\"\"Fusionne deux listes d'enregistrements historiques en fonction du champ 'Date'.\n",
    "       En cas de doublon, la donnée provenant de new_list écrase celle existante.\n",
    "    \"\"\"\n",
    "    merged = {}\n",
    "    for rec in existing_list:\n",
    "        date = rec.get(\"Date\")\n",
    "        if date:\n",
    "            merged[date] = rec\n",
    "    for rec in new_list:\n",
    "        date = rec.get(\"Date\")\n",
    "        if date:\n",
    "            merged[date] = rec\n",
    "    merged_list = list(merged.values())\n",
    "    merged_list.sort(key=lambda r: r.get(\"Date\"))\n",
    "    return merged_list\n",
    "\n",
    "def get_info_with_retry(ticker_obj, ticker, retries=2, delay=2):\n",
    "    attempts = 0\n",
    "    while attempts < retries:\n",
    "        try:\n",
    "            info = ticker_obj.info\n",
    "            return info\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la récupération des informations pour {ticker}: {e}. Réessai dans {delay} secondes...\")\n",
    "            time.sleep(delay)\n",
    "            attempts += 1\n",
    "    print(f\"⚠️ Échec de récupération des informations pour {ticker} après {retries} tentatives.\")\n",
    "    return {}\n",
    "\n",
    "def get_history_with_retry(ticker_obj, ticker, start_date, end_date, retries=5, delay=5):\n",
    "    attempts = 0\n",
    "    while attempts < retries:\n",
    "        try:\n",
    "            history_df = ticker_obj.history(start=start_date, end=end_date, interval=\"1d\")\n",
    "            return history_df\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la récupération de l'historique pour {ticker}: {e}. Réessai dans {delay} secondes...\")\n",
    "            time.sleep(delay)\n",
    "            attempts += 1\n",
    "    print(f\"⚠️ Échec de récupération de l'historique pour {ticker} après {retries} tentatives.\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    # Période pour les données historiques\n",
    "    start_date_default = \"2020-01-01\"\n",
    "    end_date = \"2025-04-03\"\n",
    "    \n",
    "    # Crée le dossier \"stocks\" s'il n'existe pas\n",
    "    os.makedirs(\"stocks\", exist_ok=True)\n",
    "\n",
    "    # Récupère la date d'aujourd'hui au format AAAAMMJJ (ex: 20250319)\n",
    "    today = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "    # Construit le nom de fichier en y ajoutant la date\n",
    "    csv_filename = os.path.join(\"stocks\", f\"sp500_stocks_info_{today}.csv\")\n",
    "    \n",
    "    # Charger les données existantes (si elles existent) et indexer par ticker\n",
    "    existing_data = {}\n",
    "    if os.path.exists(csv_filename):\n",
    "        with open(csv_filename, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                ticker = row.get(\"Ticker\")\n",
    "                if ticker:\n",
    "                    existing_data[ticker] = row\n",
    "    \n",
    "    tickers = recuperer_tickers_sp500()\n",
    "    updated_data = {}\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        print(f\"Traitement de {ticker}...\")\n",
    "        stock_obj = yf.Ticker(ticker)\n",
    "        # Récupérer les infos générales\n",
    "        info = get_info_with_retry(stock_obj, ticker, retries=5, delay=5)\n",
    "        \n",
    "        # Déterminer la période de téléchargement des historiques\n",
    "        if ticker in existing_data:\n",
    "            existing_row = existing_data[ticker]\n",
    "            historical_end_existing = existing_row.get(\"historical_end\", \"N/A\")\n",
    "            if historical_end_existing != \"N/A\" and historical_end_existing >= end_date:\n",
    "                print(f\"Les données historiques pour {ticker} sont déjà à jour.\")\n",
    "                updated_data[ticker] = existing_row\n",
    "                continue\n",
    "            else:\n",
    "                if historical_end_existing != \"N/A\":\n",
    "                    try:\n",
    "                        last_date = datetime.strptime(historical_end_existing, \"%Y-%m-%d\")\n",
    "                        new_start_date = (last_date + timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erreur lors de la conversion de historical_end pour {ticker}: {e}\")\n",
    "                        new_start_date = start_date_default\n",
    "                else:\n",
    "                    new_start_date = start_date_default\n",
    "        else:\n",
    "            new_start_date = start_date_default\n",
    "        \n",
    "        # Télécharger les données historiques manquantes\n",
    "        history_df = get_history_with_retry(stock_obj, ticker, new_start_date, end_date, retries=5, delay=5)\n",
    "        if history_df is not None:\n",
    "            history_df.reset_index(inplace=True)\n",
    "            new_history_json = history_df.to_json(orient=\"records\")\n",
    "            if not history_df.empty:\n",
    "                new_history_start = history_df[\"Date\"].min().strftime(\"%Y-%m-%d\")\n",
    "                new_history_end = history_df[\"Date\"].max().strftime(\"%Y-%m-%d\")\n",
    "            else:\n",
    "                new_history_start = \"N/A\"\n",
    "                new_history_end = \"N/A\"\n",
    "            try:\n",
    "                new_history_list = json.loads(new_history_json)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors du parsing JSON pour {ticker}: {e}\")\n",
    "                new_history_list = []\n",
    "        else:\n",
    "            new_history_list = []\n",
    "            new_history_start = \"N/A\"\n",
    "            new_history_end = \"N/A\"\n",
    "        \n",
    "        # Si le ticker existe déjà, fusionner les historiques\n",
    "        if ticker in existing_data:\n",
    "            existing_row = existing_data[ticker]\n",
    "            historical_existing = existing_row.get(\"historical\", \"N/A\")\n",
    "            if historical_existing != \"N/A\":\n",
    "                try:\n",
    "                    existing_history_list = json.loads(historical_existing)\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors du parsing de l'historique existant pour {ticker}: {e}\")\n",
    "                    existing_history_list = []\n",
    "            else:\n",
    "                existing_history_list = []\n",
    "            merged_history = merge_historical(existing_history_list, new_history_list)\n",
    "            if merged_history:\n",
    "                merged_history_start = merged_history[0].get(\"Date\", \"N/A\")\n",
    "                merged_history_end = merged_history[-1].get(\"Date\", \"N/A\")\n",
    "            else:\n",
    "                merged_history_start = \"N/A\"\n",
    "                merged_history_end = \"N/A\"\n",
    "            info[\"historical\"] = json.dumps(merged_history)\n",
    "            info[\"historical_start\"] = merged_history_start\n",
    "            info[\"historical_end\"] = merged_history_end\n",
    "        else:\n",
    "            info[\"historical\"] = json.dumps(new_history_list)\n",
    "            info[\"historical_start\"] = new_history_start\n",
    "            info[\"historical_end\"] = new_history_end\n",
    "        \n",
    "        info[\"Ticker\"] = ticker\n",
    "        updated_data[ticker] = info\n",
    "        \n",
    "        print(f\"✅ Données traitées pour {ticker}\")\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # Fusionner les données mises à jour avec celles qui n'ont pas été modifiées\n",
    "    final_data = {**existing_data, **updated_data}\n",
    "    all_stock_info = list(final_data.values())\n",
    "    \n",
    "    all_keys = set()\n",
    "    for info in all_stock_info:\n",
    "        all_keys.update(info.keys())\n",
    "    all_keys = sorted(list(all_keys))\n",
    "    \n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=all_keys)\n",
    "        writer.writeheader()\n",
    "        for info in all_stock_info:\n",
    "            writer.writerow(info)\n",
    "    \n",
    "    print(f\"✅ Fichier CSV '{csv_filename}' généré avec succès.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da9b7d5-47cd-40b3-8a8a-fbc4dfc1188d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
